{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss"
      ],
      "metadata": {
        "id": "I_YflCDRknqi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dictionary = {}\n",
        "        self.reverse_dictionary = {}\n",
        "\n",
        "        # Add the padding token\n",
        "        self.__add_to_dict('<pad>')\n",
        "\n",
        "        # Add characters and numbers to the dictionary\n",
        "        for i in range(10):\n",
        "            self.__add_to_dict(str(i))\n",
        "        for i in range(26):\n",
        "            self.__add_to_dict(chr(ord('a') + i))\n",
        "\n",
        "        # Add space and punctuation to the dictionary\n",
        "        self.__add_to_dict('.')\n",
        "        self.__add_to_dict(' ')\n",
        "\n",
        "    def __add_to_dict(self, character):\n",
        "        if character not in self.dictionary:\n",
        "            self.dictionary[character] = len(self.dictionary)\n",
        "            self.reverse_dictionary[self.dictionary[character]] = character\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return [self.dictionary[c] for c in text]\n",
        "\n",
        "    def character_to_token(self, character):\n",
        "        return self.dictionary[character]\n",
        "\n",
        "    def token_to_character(self, token):\n",
        "        return self.reverse_dictionary[token]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.dictionary)"
      ],
      "metadata": {
        "id": "_A3X_SBotTyA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch module that converts tokens into embeddings.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length)\n",
        "    Output dimension is: (batch_size, sequence_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, number_of_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(\n",
        "            num_embeddings=number_of_tokens,\n",
        "            embedding_dim=d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding_layer(x)"
      ],
      "metadata": {
        "id": "eO7foDlpm5jm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the\n",
        "    transformer's input embeddings to provide a sense of position of the sequence elements.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.positional_encoding = self.create_positional_encoding()\n",
        "\n",
        "    def create_positional_encoding(self):\n",
        "        \"\"\"\n",
        "        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize positional encoding matrix\n",
        "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
        "\n",
        "        # Calculate positional encoding for each position and each dimension\n",
        "        for pos in range(self.max_sequence_length):\n",
        "            for i in range(0, self.d_model, 2):\n",
        "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
        "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
        "\n",
        "                if i + 1 < self.d_model:\n",
        "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
        "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
        "\n",
        "        # Convert numpy array to PyTorch tensor and return it\n",
        "        return torch.from_numpy(positional_encoding).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds the positional encoding to the input embeddings at the corresponding positions.\n",
        "        \"\"\"\n",
        "        # Add positional encodings to input embeddings. The \":\" indexing ensures we only add positional encodings up\n",
        "        # to the length of the sequence in the batch. x.size(0) is the batch size, so this is a way to make sure\n",
        "        # we're not adding extra positional encodings.\n",
        "        return x + self.positional_encoding[:x.size(1), :]"
      ],
      "metadata": {
        "id": "zOccUFMFlJzK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a self attention layer.\n",
        "    This layer is used in the MultiHeadedSelfAttention module.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, head_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = head_dimension\n",
        "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the self attention.\n",
        "\n",
        "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
        "        mask dimension is: (batch_size, sequence_length)\n",
        "\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
        "        query = self.query_layer(x)\n",
        "        key = self.key_layer(x)\n",
        "        value = self.value_layer(x)\n",
        "\n",
        "        # Calculate the attention weights.\n",
        "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
        "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        # Scale the attention weights.\n",
        "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
        "\n",
        "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
        "        # This will make the softmax output 0 for these values.\n",
        "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
        "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
        "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
        "        attention_scores = self.softmax(attention_weights)\n",
        "\n",
        "        # The attention scores are multiplied by the value\n",
        "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
        "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
        "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
        "        return torch.bmm(attention_scores, value)"
      ],
      "metadata": {
        "id": "NljTHpfztgmr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a multi head attention layer.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, number_of_heads):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = embedding_dimension // number_of_heads\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        # Create the self attention modules\n",
        "        self.self_attentions = torch.nn.ModuleList(\n",
        "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
        "\n",
        "        # Create a linear layer to combine the outputs of the self attention modules\n",
        "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the multi head attention.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        mask dimensions are: (batch_size, sequence_length)\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "        # Compute the self attention for each head\n",
        "        # self_attention_outputs dimensions are:\n",
        "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
        "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
        "\n",
        "        # Concatenate the self attention outputs\n",
        "        # self_attention_outputs_concatenated dimensions are:\n",
        "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
        "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
        "\n",
        "        # Apply the output layer to the concatenated self attention outputs\n",
        "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        return self.output_layer(concatenated_self_attention_outputs)"
      ],
      "metadata": {
        "id": "vxFvBC6CtnzR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for an encoder layer.\n",
        "\n",
        "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
        "\n",
        "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
        "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
        "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Compute the encoder layer.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        mask dimensions are: (batch_size, sequence_length)\n",
        "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # Layer normalization 1\n",
        "        normalized_x = self.layer_normalization_1(x)\n",
        "\n",
        "        # Multi headed self attention\n",
        "        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
        "\n",
        "        # Residual output\n",
        "        residual_output = x + attention_output\n",
        "\n",
        "        # Layer normalization 2\n",
        "        normalized_residual_output = self.layer_normalization_2(residual_output)\n",
        "\n",
        "        # Feed forward\n",
        "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
        "\n",
        "        # Dropout, only when training.\n",
        "        if self.training:\n",
        "            feed_forward_output = self.dropout(feed_forward_output)\n",
        "\n",
        "        # Residual output\n",
        "        return residual_output + feed_forward_output"
      ],
      "metadata": {
        "id": "eam9wklznJkq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderStack(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a stack of decoders.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_layers,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate,\n",
        "            max_sequence_length\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "        # Create the encoder layers\n",
        "        self.encoder_layers = torch.nn.ModuleList(\n",
        "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
        "             range(number_of_layers)])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        decoder_outputs = x\n",
        "        for decoder_layer in self.encoder_layers:\n",
        "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
        "\n",
        "        return decoder_outputs"
      ],
      "metadata": {
        "id": "2JbjavtVnCAC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a feed forward layer.\n",
        "\n",
        "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
        "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the feed forward layer.\n",
        "        \"\"\"\n",
        "        return self.linear_2(torch.relu(self.linear_1(x)))"
      ],
      "metadata": {
        "id": "EzWWJRjtocS1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for the language model head.\n",
        "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, number_of_tokens):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_tokens = number_of_tokens\n",
        "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute the language model head.\n",
        "\n",
        "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
        "        \"\"\"\n",
        "        # Compute the linear layer\n",
        "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
        "        linear_output = self.linear(x)\n",
        "\n",
        "        return linear_output"
      ],
      "metadata": {
        "id": "lVslR4KsnQFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vP-dBVCxgI5u"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module for a language model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            number_of_tokens,  # The number of tokens in the vocabulary\n",
        "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
        "            embedding_dimension=512,  # The dimension of the token embeddings\n",
        "            number_of_layers=6,  # The number of decoder layers to use\n",
        "            number_of_heads=4,  # The number of attention heads to use\n",
        "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
        "            dropout_rate=0.1  # The dropout rate to use\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.number_of_tokens = number_of_tokens\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        if feed_forward_dimension is None:\n",
        "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
        "            self.feed_forward_dimension = embedding_dimension * 4\n",
        "        else:\n",
        "            self.feed_forward_dimension = feed_forward_dimension\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Create the token embedding layer\n",
        "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
        "\n",
        "        # Create the positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
        "\n",
        "        # Create the normalization layer\n",
        "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "        # Create the decoder stack\n",
        "        self.decoder = DecoderStack(\n",
        "            embedding_dimension=embedding_dimension,\n",
        "            number_of_layers=number_of_layers,\n",
        "            number_of_heads=number_of_heads,\n",
        "            feed_forward_dimension=self.feed_forward_dimension,\n",
        "            dropout_rate=dropout_rate,\n",
        "            max_sequence_length=max_sequence_length\n",
        "        )\n",
        "\n",
        "        # Create the language model head\n",
        "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Compute the token embeddings\n",
        "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "\n",
        "        # Compute the positional encoding\n",
        "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
        "        positional_encoding = self.positional_encoding(token_embeddings)\n",
        "\n",
        "        # Post embedding layer normalization\n",
        "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
        "\n",
        "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
        "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
        "\n",
        "        return lm_head_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoregressiveWrapper(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gpt_model):\n",
        "        super().__init__()\n",
        "        self.model = gpt_model\n",
        "        self.max_sequence_length = self.model.max_sequence_length\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Autoregressive forward pass\n",
        "        \"\"\"\n",
        "        inp, target = x[:, :-1], x[:, 1:]\n",
        "        mask = mask[:, :-1]\n",
        "\n",
        "        output = self.model(inp, mask)\n",
        "        return output, target\n",
        "\n",
        "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Calculate the token probabilities for the next token in the sequence.\n",
        "        \"\"\"\n",
        "        logits = self.model(x, mask)[:, -1]\n",
        "\n",
        "        # Apply the temperature\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        # Apply the softmax\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "NLFbEZBqnvHc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        if optimizer is None:\n",
        "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        else:\n",
        "            self.optimizer = optimizer\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, data: List[str], epochs, batch_size):\n",
        "        loss_per_epoch = []\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "\n",
        "            # Shuffle the sequences\n",
        "            random.shuffle(data)\n",
        "\n",
        "            # Create batches of sequences and their respective mask.\n",
        "            batches = []\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
        "\n",
        "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
        "                mask_tensor = torch.ones_like(sequence_tensor)\n",
        "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')] = 0\n",
        "\n",
        "                batches.append((sequence_tensor, mask_tensor))\n",
        "\n",
        "            # Train the model on each batch\n",
        "            for batch in batches:\n",
        "                self.model.train()\n",
        "\n",
        "                # Create the input and mask tensors\n",
        "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "\n",
        "                for i, input_entry in enumerate(batch[0]):\n",
        "                    input_tensor[i] = input_entry\n",
        "\n",
        "                for i, mask_entry in enumerate(batch[1]):\n",
        "                    mask_tensor[i] = mask_entry\n",
        "\n",
        "                # Compute the model output\n",
        "                model_output, target = self.model.forward(x=input_tensor, mask=mask_tensor)\n",
        "\n",
        "                # Compute the losses\n",
        "                # The loss is computed on the model output and the target\n",
        "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
        "\n",
        "                # Backpropagate the loss.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the gradients. This is used to prevent exploding gradients.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "\n",
        "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
        "                # are not used in the next step.\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            # Print the loss\n",
        "            epoch_loss = np.average(losses)\n",
        "            loss_per_epoch.append(epoch_loss)\n",
        "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
        "\n",
        "        return loss_per_epoch"
      ],
      "metadata": {
        "id": "sJDLUzVrolqf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
        "    # Create sequences of length max_sequence_length + 1\n",
        "    # The last token of each sequence is the target token\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
        "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
        "    # Tokenize the training data\n",
        "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
        "    for _ in range(max_sequence_length):\n",
        "        # Prepend padding tokens\n",
        "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
        "    return tokenized_training_data\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "embedding_dimension = 256\n",
        "max_sequence_length = 20\n",
        "number_of_tokens = tokenizer.size()\n",
        "\n",
        "# Create the model\n",
        "model = AutoregressiveWrapper(LanguageModel(\n",
        "    embedding_dimension=embedding_dimension,\n",
        "    number_of_tokens=number_of_tokens,\n",
        "    number_of_heads=4,\n",
        "    number_of_layers=3,\n",
        "    dropout_rate=0.1,\n",
        "    max_sequence_length=max_sequence_length\n",
        "))\n",
        "\n",
        "# Create the training data\n",
        "training_data = '. '.join([\n",
        "    'cats rule the world',\n",
        "    'dogs are the best',\n",
        "    'elephants have long trunks',\n",
        "    'monkeys like bananas',\n",
        "    'pandas eat bamboo',\n",
        "    'tigers are dangerous',\n",
        "    'zebras have stripes',\n",
        "    'lions are the kings of the savannah',\n",
        "    'giraffes have long necks',\n",
        "    'hippos are big and scary',\n",
        "    'rhinos have horns',\n",
        "    'penguins live in the arctic',\n",
        "    'polar bears are white'\n",
        "])\n",
        "\n",
        "tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
        "sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
        "\n",
        "# Train the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "trainer = Trainer(model, tokenizer, optimizer)\n",
        "trainer.train(sequences, epochs=40, batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4nvN-INkgGC",
        "outputId": "04aadcf0-edb8-4983-a8a8-65925f64f5a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 2.4396827740546985\n",
            "Epoch: 1 Loss: 1.757459350121327\n",
            "Epoch: 2 Loss: 1.4867284771723626\n",
            "Epoch: 3 Loss: 1.2543266828243549\n",
            "Epoch: 4 Loss: 1.0496051326776161\n",
            "Epoch: 5 Loss: 0.8916665514310201\n",
            "Epoch: 6 Loss: 0.7545514985536917\n",
            "Epoch: 7 Loss: 0.6476415754892887\n",
            "Epoch: 8 Loss: 0.5562718579402337\n",
            "Epoch: 9 Loss: 0.444701339953985\n",
            "Epoch: 10 Loss: 0.3632149203465535\n",
            "Epoch: 11 Loss: 0.2946324424865918\n",
            "Epoch: 12 Loss: 0.24361024758754632\n",
            "Epoch: 13 Loss: 0.1952084352572759\n",
            "Epoch: 14 Loss: 0.16574987157797202\n",
            "Epoch: 15 Loss: 0.14641634967082587\n",
            "Epoch: 16 Loss: 0.11776513759142314\n",
            "Epoch: 17 Loss: 0.10830591466182317\n",
            "Epoch: 18 Loss: 0.09149824359860176\n",
            "Epoch: 19 Loss: 0.08468510205738056\n",
            "Epoch: 20 Loss: 0.07831152748221006\n",
            "Epoch: 21 Loss: 0.07285649458376262\n",
            "Epoch: 22 Loss: 0.0684501145226069\n",
            "Epoch: 23 Loss: 0.07092983910861687\n",
            "Epoch: 24 Loss: 0.05418227068506754\n",
            "Epoch: 25 Loss: 0.051958173943253666\n",
            "Epoch: 26 Loss: 0.04598209760987606\n",
            "Epoch: 27 Loss: 0.045132579401326485\n",
            "Epoch: 28 Loss: 0.03889825565215105\n",
            "Epoch: 29 Loss: 0.039310082900696076\n",
            "Epoch: 30 Loss: 0.032501186638210826\n",
            "Epoch: 31 Loss: 0.03222631436223403\n",
            "Epoch: 32 Loss: 0.0404384865735968\n",
            "Epoch: 33 Loss: 0.0383986003864079\n",
            "Epoch: 34 Loss: 0.02740785864014656\n",
            "Epoch: 35 Loss: 0.029114041955043107\n",
            "Epoch: 36 Loss: 0.022472801946628936\n",
            "Epoch: 37 Loss: 0.0221390427233508\n",
            "Epoch: 38 Loss: 0.030452151126108874\n",
            "Epoch: 39 Loss: 0.01946955632704955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4396827740546985,\n",
              " 1.757459350121327,\n",
              " 1.4867284771723626,\n",
              " 1.2543266828243549,\n",
              " 1.0496051326776161,\n",
              " 0.8916665514310201,\n",
              " 0.7545514985536917,\n",
              " 0.6476415754892887,\n",
              " 0.5562718579402337,\n",
              " 0.444701339953985,\n",
              " 0.3632149203465535,\n",
              " 0.2946324424865918,\n",
              " 0.24361024758754632,\n",
              " 0.1952084352572759,\n",
              " 0.16574987157797202,\n",
              " 0.14641634967082587,\n",
              " 0.11776513759142314,\n",
              " 0.10830591466182317,\n",
              " 0.09149824359860176,\n",
              " 0.08468510205738056,\n",
              " 0.07831152748221006,\n",
              " 0.07285649458376262,\n",
              " 0.0684501145226069,\n",
              " 0.07092983910861687,\n",
              " 0.05418227068506754,\n",
              " 0.051958173943253666,\n",
              " 0.04598209760987606,\n",
              " 0.045132579401326485,\n",
              " 0.03889825565215105,\n",
              " 0.039310082900696076,\n",
              " 0.032501186638210826,\n",
              " 0.03222631436223403,\n",
              " 0.0404384865735968,\n",
              " 0.0383986003864079,\n",
              " 0.02740785864014656,\n",
              " 0.029114041955043107,\n",
              " 0.022472801946628936,\n",
              " 0.0221390427233508,\n",
              " 0.030452151126108874,\n",
              " 0.01946955632704955]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_left(sequence, final_length, padding_token):\n",
        "    return [padding_token] * (final_length - len(sequence)) + sequence\n",
        "\n",
        "\n",
        "class Generator:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            model,\n",
        "            tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate(\n",
        "            self,\n",
        "            max_tokens_to_generate: int,\n",
        "            prompt: str = None,\n",
        "            temperature: float = 1.0,\n",
        "            eos_token: int = None,\n",
        "            padding_token: int = 0):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        if prompt is None:\n",
        "            start_tokens = [self.tokenizer.character_to_token(padding_token)]\n",
        "        else:\n",
        "            start_tokens = self.tokenizer.tokenize(prompt)\n",
        "\n",
        "        input_tensor = torch.tensor(\n",
        "            pad_left(\n",
        "                sequence=start_tokens,\n",
        "                final_length=self.model.max_sequence_length + 1,\n",
        "                padding_token=padding_token\n",
        "            ),\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        num_dims = len(input_tensor.shape)\n",
        "\n",
        "        if num_dims == 1:\n",
        "            input_tensor = input_tensor[None, :]\n",
        "\n",
        "        out = input_tensor\n",
        "        for _ in range(max_tokens_to_generate):\n",
        "\n",
        "            x = out[:, -self.model.max_sequence_length:]\n",
        "\n",
        "            mask = torch.ones_like(x)\n",
        "            mask[x == padding_token] = 0\n",
        "\n",
        "            # Compute the next token probabilities\n",
        "            next_token_probabilities = self.model.next_token_probabilities(\n",
        "                x=x,\n",
        "                temperature=temperature,\n",
        "                mask=mask\n",
        "            )\n",
        "\n",
        "            # Sample the next token from the probability distribution\n",
        "            next_token = torch.multinomial(next_token_probabilities, num_samples=1)\n",
        "\n",
        "            # Append the next token to the output\n",
        "            out = torch.cat([out, next_token], dim=1)\n",
        "\n",
        "            # If the end of sequence token is reached, stop generating tokens\n",
        "            if eos_token is not None and next_token == eos_token:\n",
        "                break\n",
        "\n",
        "        generated_tokens = out[0].tolist()\n",
        "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])\n",
        "\n"
      ],
      "metadata": {
        "id": "nHpC3wZpnonl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens_to_generate = 15\n",
        "generator = Generator(model, tokenizer)\n",
        "generated_text = generator.generate(\n",
        "    max_tokens_to_generate=max_tokens_to_generate,\n",
        "    prompt=\"cats\",\n",
        "    padding_token=tokenizer.character_to_token('<pad>')\n",
        ")\n",
        "print(generated_text.replace('<pad>', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHBx8bygs4zH",
        "outputId": "55a86968-a6c3-45d6-95d1-5fc74752b87e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats rule the world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tOf4yWYS1NY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}